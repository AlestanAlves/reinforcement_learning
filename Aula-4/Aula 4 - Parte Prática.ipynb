{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4 - Parte Prática - Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nesse quarto notebook vamos estudar um dos algoritmos mais implementados da família dos *Policy Gradients*. Ao implementar o A2C, você terá contato com importantes conceitos utilizados em *Deep RL*. Em particular, utilizaremos pela primeira vez no curso redes neurais com *features* compartilhadas e ambientes vetorizados (i.e., paralelizados) para coleta de dados mais eficiente.\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Familiarizar-se com os componentes *Actor* e *Critic*\n",
    "- Entender o papel da função Valor na estimativa truncada dos retornos\n",
    "- Ter um primeiro contato com truques de implementação tipicamente utilizados e RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "> **Atenção:** não se esqueça de executar todos os `imports` necessários antes prosseguir com o tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in /home/alestan/miniconda3/envs/rlsummer/lib/python3.6/site-packages (0.16.0)\n",
      "Requirement already satisfied: scipy in /home/alestan/.local/lib/python3.6/site-packages (from gym[box2d]) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/alestan/miniconda3/envs/rlsummer/lib/python3.6/site-packages (from gym[box2d]) (1.2.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/alestan/miniconda3/envs/rlsummer/lib/python3.6/site-packages (from gym[box2d]) (1.4.10)\n",
      "Requirement already satisfied: six in /home/alestan/.local/lib/python3.6/site-packages (from gym[box2d]) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/alestan/.local/lib/python3.6/site-packages (from gym[box2d]) (1.18.1)\n",
      "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /home/alestan/miniconda3/envs/rlsummer/lib/python3.6/site-packages (from gym[box2d]) (2.3.8)\n",
      "Requirement already satisfied: future in /home/alestan/miniconda3/envs/rlsummer/lib/python3.6/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/alestan/miniconda3/envs/rlsummer/lib/python3.6/site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/alestan/.local/lib/python3.6/site-packages (from pydot) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(null);\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import pydot \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils.agent import RLAgent, RandomAgent\n",
    "from utils.memory import OnPolicyReplay\n",
    "from utils.networks import build_actor_critic_network\n",
    "import utils.runner\n",
    "from utils.viz import *\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")     # ignore TensorFlow warnings\n",
    "gym.logger.set_level(logging.ERROR)   # ignore OpenAI Gym warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. LunarLander-v2\n",
    "\n",
    "Para o notebook de hoje utilizaremos um outro problema do Gym que é mais desafiador que o CartPole.\n",
    "\n",
    "\n",
    "> **Atenção**: para entender melhor a tarefa leia a documentação do LunarLander disponível em http://gym.openai.com/envs/LunarLander-v2/.\n",
    "\n",
    "Execute o código abaixo para visualizar alguns episódios do agente aleatório para ter uma melhor ideia da tarefa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> episode = 0 / 30, total_reward =  -353.7703, episode_length = 109\n",
      ">> episode = 1 / 30, total_reward =  -275.1834, episode_length = 107\n",
      ">> episode = 2 / 30, total_reward =  -162.6296, episode_length = 98\n",
      ">> episode = 3 / 30, total_reward =  -122.2536, episode_length = 94\n",
      ">> episode = 4 / 30, total_reward =  -373.8535, episode_length = 100\n",
      ">> episode = 5 / 30, total_reward =   -48.1675, episode_length = 70\n",
      ">> episode = 6 / 30, total_reward =  -276.1076, episode_length = 111\n",
      ">> episode = 7 / 30, total_reward =  -134.2783, episode_length = 123\n",
      ">> episode = 8 / 30, total_reward =   -39.1867, episode_length = 125\n",
      ">> episode = 9 / 30, total_reward =  -212.1545, episode_length = 66\n",
      ">> episode = 10 / 30, total_reward =   -69.1504, episode_length = 89\n",
      ">> episode = 11 / 30, total_reward =  -177.7158, episode_length = 71\n",
      ">> episode = 12 / 30, total_reward =   -32.1991, episode_length = 65\n",
      ">> episode = 13 / 30, total_reward =  -127.6573, episode_length = 60\n",
      ">> episode = 14 / 30, total_reward =   -82.9162, episode_length = 58\n",
      ">> episode = 15 / 30, total_reward =  -278.2957, episode_length = 73\n",
      ">> episode = 16 / 30, total_reward =  -384.3373, episode_length = 88\n",
      ">> episode = 17 / 30, total_reward =  -122.8848, episode_length = 79\n",
      ">> episode = 18 / 30, total_reward =   -75.2157, episode_length = 107\n",
      ">> episode = 19 / 30, total_reward =   -81.7356, episode_length = 105\n",
      ">> episode = 20 / 30, total_reward =  -340.9844, episode_length = 143\n",
      ">> episode = 21 / 30, total_reward =  -105.8385, episode_length = 74\n",
      ">> episode = 22 / 30, total_reward =  -200.6283, episode_length = 115\n",
      ">> episode = 23 / 30, total_reward =   -92.8629, episode_length = 76\n",
      ">> episode = 24 / 30, total_reward =  -342.2291, episode_length = 106\n",
      ">> episode = 25 / 30, total_reward =  -280.9008, episode_length = 88\n",
      ">> episode = 26 / 30, total_reward =  -125.4386, episode_length = 131\n",
      ">> episode = 27 / 30, total_reward =   -91.1003, episode_length = 63\n",
      ">> episode = 28 / 30, total_reward =   -29.1855, episode_length = 88\n",
      ">> episode = 29 / 30, total_reward =  -146.8879, episode_length = 101\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "agent = RandomAgent(env.observation_space, env.action_space, None)\n",
    "\n",
    "utils.runner.evaluate(agent, env, n_episodes=30, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ambientes vetorizados no Gym\n",
    "\n",
    "Pela primeira vez no curso estaremos utilizando ambientes vetorizados, isto é, que emulam o comportamente de vários ambientes sendo executados em paralelo.\n",
    "\n",
    "Execute o código abaixo e tente entender como o ambiente retornado pelo `gym.vector.make` se comporta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ForkProcess(Worker<AsyncVectorEnv>-0, started daemon)>, <ForkProcess(Worker<AsyncVectorEnv>-1, started daemon)>, <ForkProcess(Worker<AsyncVectorEnv>-2, started daemon)>, <ForkProcess(Worker<AsyncVectorEnv>-3, started daemon)>]\n",
      "Box(4, 8) Box(8,)\n",
      "Tuple(Discrete(4), Discrete(4), Discrete(4), Discrete(4)) Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.vector.make(\"LunarLander-v2\", num_envs=4, asynchronous=True)\n",
    "\n",
    "print(env.processes)\n",
    "\n",
    "print(env.observation_space, env.single_observation_space)\n",
    "print(env.action_space, env.single_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> step = 4\n",
      "observations =\n",
      "[[ 0.00501194  1.4019997   0.25347212 -0.21106076 -0.00573805 -0.0568253\n",
      "   0.          0.        ]\n",
      " [ 0.00604897  1.404977    0.3059201  -0.14490628 -0.00692669 -0.068583\n",
      "   0.          0.        ]\n",
      " [-0.00178061  1.3876907  -0.09474613 -0.5289163   0.00355007  0.05096614\n",
      "   0.          0.        ]\n",
      " [-0.00424309  1.3905098  -0.20881315 -0.46624473  0.00301327  0.01007152\n",
      "   0.          0.        ]]\n",
      "actions = (0, 0, 1, 3)\n",
      "rewards = [-1.36201025 -0.99585481 -1.73155803 -0.89997399]\n",
      "dones = [False False False False]\n",
      "\n",
      ">> step = 8\n",
      "observations =\n",
      "[[ 0.0075593   1.397739    0.25742063 -0.18937247 -0.00839923 -0.05322827\n",
      "   0.          0.        ]\n",
      " [ 0.00916634  1.4011133   0.31755942 -0.1717561  -0.01268592 -0.11519512\n",
      "   0.          0.        ]\n",
      " [-0.00279255  1.3751909  -0.106071   -0.55556864  0.00836617  0.09633093\n",
      "   0.          0.        ]\n",
      " [-0.00622578  1.3800924  -0.199267   -0.46300352  0.00400537  0.01984377\n",
      "   0.          0.        ]]\n",
      "actions = (2, 3, 1, 2)\n",
      "rewards = [ 0.88548332 -2.47406217 -2.08863122  1.32236191]\n",
      "dones = [False False False False]\n",
      "\n",
      ">> step = 12\n",
      "observations =\n",
      "[[ 0.01017418  1.393557    0.26387542 -0.18588929 -0.01075422 -0.04710435\n",
      "   0.          0.        ]\n",
      " [ 0.0121212   1.3972901   0.30212522 -0.16998625 -0.01924915 -0.13127676\n",
      "   0.          0.        ]\n",
      " [-0.00380478  1.3620913  -0.10608556 -0.58224183  0.01318088  0.09630342\n",
      "   0.          0.        ]\n",
      " [-0.00811501  1.3690752  -0.18754631 -0.48964512  0.00264558 -0.02719828\n",
      "   0.          0.        ]]\n",
      "actions = (2, 2, 0, 3)\n",
      "rewards = [-0.43930545  0.86068182 -1.79412685 -0.82037596]\n",
      "dones = [False False False False]\n",
      "\n",
      ">> num_envs = 4, n_steps_per_env = 3, total timesteps = 12\n"
     ]
    }
   ],
   "source": [
    "n_steps_per_env = 3\n",
    "\n",
    "observations = env.reset()\n",
    "\n",
    "step = 0\n",
    "\n",
    "for _ in range(n_steps_per_env):\n",
    "    actions = env.action_space.sample()\n",
    "    observations, rewards, dones, _ = env.step(actions)\n",
    "    step += len(observations)\n",
    "\n",
    "    print(f\">> step = {step}\")\n",
    "    print(f\"observations =\\n{observations}\")\n",
    "    print(f\"actions = {actions}\")\n",
    "    print(f\"rewards = {rewards}\")\n",
    "    print(f\"dones = {dones}\")\n",
    "    print()\n",
    "    \n",
    "print(f\">> num_envs = {env.num_envs}, n_steps_per_env = {n_steps_per_env}, total timesteps = {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Asychronous Actor-Critic (A2C)\n",
    "\n",
    "<img src=\"img/a2c-algo.png\" alt=\"A2C Algorithm\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "> **Observação**: para uma introdução mais intuitiva do A2C recomendamos o blog post https://sudonull.com/post/32170-Intuitive-RL-Reinforcement-Learning-Introduction-to-Advantage-Actor-Critic-A2C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compartilhando informação entre *Actor* e *Critic*: *2-head model*\n",
    "\n",
    "Na arquitetura A2C é comum implementar o Actor e o Critic em um mesmo modelo que compartilha parâmetros. Execute o código abaixo e tente interpretar a figura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 8\n",
    "env = gym.vector.make(\"LunarLander-v2\", num_envs=num_envs, asynchronous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"actor_critic_net\": {\n",
    "        \"hidden_layers\": [64, 64],\n",
    "        \"activation\": \"tanh\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "actor_critic_net = build_actor_critic_network(env.single_observation_space, env.single_action_space, config[\"actor_critic_net\"])\n",
    "tf.keras.utils.plot_model(actor_critic_net, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Otimizando a política e a função Valor via *joint loss*\n",
    "\n",
    "$$\n",
    "[\\theta, \\phi] \\leftarrow [\\theta, \\phi] + \\alpha \\nabla_{\\theta, \\phi}(L_{actor}(\\theta) + L_{critic}(\\phi))\n",
    "$$\n",
    "\n",
    "onde $L_{actor}(\\theta)$ e $L_{critic}(\\phi)$ correspondem respectivamente ao *policy loss* e *mean squared error*:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{actor}(\\theta) &= - \\frac{1}{K} \\sum_{t=1}^K \\log \\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{s}_t) \\hat{A}_t^{(n)} \\\\\n",
    "L_{critic}(\\phi) &= \\frac{1}{K} \\sum_{t=1}^K  (V_{\\phi}(\\mathbf{s}_t) - \\hat{R}_t)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> **<font color=\"red\">IMPORTANTE</color>**: Note que para o problema de regressão que precisamos resolver para aprender o *critic*, o retorno descontado $\\hat{R}_t$ funciona como o *target* e, portanto, deve ser considerado uma \"constante\" para o TensorFlow. No exercício abaixo você precisará se lembrar disso!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Bônus de exploração: entropia da distribuição de ações\n",
    "\n",
    "Além do *joint loss* adicionaremos também um outro termo à função objetivo a fim de incentivar o agente a continuar explorando novas ações.\n",
    "\n",
    "$$\n",
    "H(\\pi_{\\theta}) = \\mathbb{E}_{\\mathbf{s}, \\mathbf{a} \\sim \\pi_{\\theta}} \\left [ - \\log \\pi_{\\theta}(\\mathbf{a}|\\mathbf{s})\\right]\n",
    "$$\n",
    "\n",
    "Tente entender no código abaixo onde e como esse bônus da entropia é implementado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">EXERCÍCIO-PROGRAMA:</font>**\n",
    "\n",
    "Nesse exercício você deverá implementar o *value function loss* no método `_joint_loss_fn` da classe abaixo.\n",
    "\n",
    "\n",
    "> **Nota 1**: consulte a documentação do MSE em https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError. Será útil!\n",
    "\n",
    "> **Nota 2**: você precisará utilizar a função `tf.stop_gradient` no cálculo do *target* na loss do Value Function. Isso permite que o TensorFlow não tente diferenciar o *target*. Veja nota <font color=\"red\">IMPORTANTE</color> na Seção 2.2.\n",
    "\n",
    "> **Nota 3**: se você tiver algum erro de \"tipos\", por exemplo, esperava-se `float32` em algum ponto do código mas o array do *NumPy* foi calculado como `float64` você poderá fazer *casting* manual de tipos usando o método de `astype(\"f\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        super().__init__(obs_space, action_space, config)\n",
    "        \n",
    "        self.memory = OnPolicyReplay()\n",
    "        self.actor_critic = build_actor_critic_network(obs_space, action_space, config[\"actor_critic_net\"])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                0.00083,\n",
    "                decay_steps=config[\"total_timesteps\"] / config[\"train_batch_size\"],\n",
    "                end_learning_rate=1e-4,\n",
    "                power=1.0\n",
    "            ))\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação para ser tomada dada uma observação do ambiente.\n",
    "        \n",
    "        Args: \n",
    "            obs: observação do ambiente.\n",
    "        \n",
    "        Return:\n",
    "            action: ação válida dentro do espaço de ações.\n",
    "        \"\"\"\n",
    "        return self._act(obs).numpy()\n",
    "        \n",
    "    @tf.function\n",
    "    def _act(self, obs):\n",
    "        action_dist, _ = self.actor_critic(obs)\n",
    "        return action_dist.sample()\n",
    "    \n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"\n",
    "        Registra na memória do agente uma transição do ambiente.\n",
    "\n",
    "        Args:\n",
    "            obs:            observação do ambiente antes da execução da ação.\n",
    "            action:         ação escolhida pelo agente.\n",
    "            reward (float): escalar indicando a recompensa obtida após a execução da ação.\n",
    "            next_obs:       nova observação recebida do ambiente após a execução da ação.\n",
    "            done (bool):    True se a nova observação corresponde a um estado terminal, False caso contrário.\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Método de treinamento do agente. A partir das experiências de sua memória,\n",
    "        o agente aprende um novo comportamento.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.memory.batch_size < self.config[\"train_batch_size\"]:\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample()\n",
    "        weights = self.actor_critic.trainable_weights\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_loss, vf_loss, entropy_loss = self._joint_loss_fn(batch)\n",
    "            loss = policy_loss + self.config[\"vf_loss_coeff\"] * tf.cast(vf_loss, tf.float32) - self.config[\"entropy_coeff\"] * entropy_loss\n",
    "            gradients = tape.gradient(loss, weights)\n",
    "    \n",
    "        gradients = tuple(tf.clip_by_norm(grad, clip_norm=0.5) for grad in gradients)\n",
    "        self.optimizer.apply_gradients(zip(gradients, weights))\n",
    "      \n",
    "        return {\n",
    "            \"policy_loss\": policy_loss.numpy(),\n",
    "            \"vf_loss\": vf_loss.numpy(),\n",
    "            \"entropy_loss\": entropy_loss.numpy()\n",
    "        }\n",
    "\n",
    "    def _joint_loss_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Calcula a função loss do policy gradients para um `batch` de transições.\n",
    "        \n",
    "        Um `batch` agrega arrays n-dimensionais. Cada array (e.g., batch[\"states\"],\n",
    "        batch[\"actions\"], batch[\"rewards\"]) tem como primeiras duas dimensões o número\n",
    "        de passos dados no ambiente vetorizado e o número de ambientes em paralelo. \n",
    "        Por exemplo, batch[\"states\"][t][k] devolve um array correspondendo ao estado \n",
    "        no passo t devolvido pelo k-ésimo ambiente.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, np.ndarray]): dicionário para acesso às matrizes de \n",
    "                estados, ações, recompensas, próximos estados e flags de terminação. \n",
    "        \n",
    "        Return:\n",
    "            loss (tf.Tensor): surrogate loss conjunta da política, função valor e\n",
    "                bônus de entropia.\n",
    "        \"\"\"\n",
    "        states = batch[\"states\"]\n",
    "        actions = batch[\"actions\"]\n",
    "        rewards = batch[\"rewards\"]\n",
    "        next_states = batch[\"next_states\"]\n",
    "        dones = batch[\"dones\"]\n",
    "\n",
    "        n_steps = len(states)\n",
    "        gamma = self.config[\"gamma\"]\n",
    "        lambda_ = self.config[\"lambda\"]\n",
    "\n",
    "        action_dists, values = self.actor_critic(states)\n",
    "        _, last_value = self.actor_critic(next_states[-1:])\n",
    "\n",
    "        values = tf.squeeze(tf.concat([values, last_value], axis=0))\n",
    "        values, next_values = values[:-1], values[1:]\n",
    "\n",
    "        deltas = rewards + gamma * (1 - dones) * next_values - values\n",
    "\n",
    "        returns = np.empty_like(rewards)\n",
    "        advantages = np.empty_like(rewards)\n",
    "\n",
    "        returns[-1] = rewards[-1] + gamma * (1 - dones[-1]) * next_values[-1]\n",
    "        advantages[-1] = deltas[-1]\n",
    "\n",
    "        for t in reversed(range(n_steps - 1)):\n",
    "            returns[t] = rewards[t] + gamma * (1 - dones[t]) * returns[t+1]\n",
    "            advantages[t] = deltas[t] + (gamma * lambda_) * (1 - dones[t]) * advantages[t+1]\n",
    "\n",
    "        log_probs = action_dists.log_prob(actions)\n",
    "\n",
    "        policy_loss = - tf.reduce_sum(log_probs * tf.stop_gradient(advantages.astype(\"f\")))\n",
    "        \n",
    "        # SEU CÓDIGO AQUI =====================================\n",
    "        \n",
    "        vf_loss = None\n",
    "        \n",
    "        env = gym.make('LunarLander-v2')\n",
    "        for i_episode in range(10):\n",
    "            observation = env.reset()\n",
    "            for t in range(100):\n",
    "                env.render()\n",
    "                #print(observation)\n",
    "                action = env.action_space.sample()\n",
    "                observation, reward, done, info = env.step(action)\n",
    "        env.close()\n",
    "    \n",
    "        # =====================================================\n",
    "        entropy_loss = tf.reduce_mean(action_dists.entropy())\n",
    "\n",
    "        return policy_loss, vf_loss, entropy_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar o seu agente A2C execute o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1e2dd9024ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA2C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_observation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_action_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_total_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_total_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/reinforcement_learning/Aula-4/utils/runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, total_timesteps)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mtimestep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8796835677f1>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvf_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vf_loss_coeff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvf_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entropy_coeff\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    703\u001b[0m       \u001b[0;31m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0;31m# strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    315\u001b[0m                                          as_ref=False):\n\u001b[1;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rlsummer/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "total_timesteps = 1_000_000\n",
    "\n",
    "num_envs = 8\n",
    "train_env = gym.vector.make(\"LunarLander-v2\", num_envs=num_envs, asynchronous=True)\n",
    "\n",
    "config = {\n",
    "    \"actor_critic_net\": {\n",
    "        \"hidden_layers\": [64, 64],\n",
    "        \"activation\": \"tanh\"\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"class_name\": \"RMSprop\",\n",
    "        \"config\": {\n",
    "            \"learning_rate\": 8e-4,\n",
    "            \"rho\": 0.99\n",
    "        }\n",
    "    },\n",
    "    \"total_timesteps\": total_timesteps,\n",
    "    \"train_batch_size\": 40,\n",
    "    \"gamma\": 0.995,\n",
    "    \"lambda\": 1.0,\n",
    "    \"vf_loss_coeff\": 0.25,\n",
    "    \"entropy_coeff\": 1e-5\n",
    "}\n",
    "\n",
    "agent = A2C(train_env.single_observation_space, train_env.single_action_space, config)\n",
    "\n",
    "timesteps, avg_total_rewards, losses = utils.runner.train(agent, train_env, total_timesteps)\n",
    "\n",
    "plot_metrics(avg_total_rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar o agente treinado execute o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> episode = 0 / 10, total_reward =  -144.9840, episode_length = 94\n",
      ">> episode = 1 / 10, total_reward =  -531.1152, episode_length = 108\n",
      ">> episode = 2 / 10, total_reward =   -85.4648, episode_length = 69\n",
      ">> episode = 3 / 10, total_reward =  -118.7662, episode_length = 73\n",
      ">> episode = 4 / 10, total_reward =  -116.4822, episode_length = 88\n",
      ">> episode = 5 / 10, total_reward =  -390.7390, episode_length = 74\n",
      ">> episode = 6 / 10, total_reward =   -89.6952, episode_length = 57\n",
      ">> episode = 7 / 10, total_reward =  -205.2787, episode_length = 79\n",
      ">> episode = 8 / 10, total_reward =  -568.8253, episode_length = 93\n",
      ">> episode = 9 / 10, total_reward =   -28.7382, episode_length = 68\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "utils.runner.evaluate(agent, eval_env, n_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">PARABÉNS!</font>**\n",
    "\n",
    "\n",
    "Se você conseguiu chegar até aqui, parabéns! Você conseguiu fazer funcionar uma primeira versão do A2C. Sabemos que isso não é uma tarefa fácil... :)\n",
    "\n",
    "Esperamos que você tenha se familiarizado com as principais ideias por trás de um algoritmo de Deep RL e tenha vivenciado um pouco a dificuladade de treinar agentes de RL, mesmo para problemas aparentemente simples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
